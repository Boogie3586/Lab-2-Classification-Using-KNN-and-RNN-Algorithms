{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb102d-0fbd-47cd-9041-ac698a467648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and Prepare the Dataset\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Convert to DataFrame for exploration\n",
    "df = pd.DataFrame(X, columns=wine.feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# Basic data exploration\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\\n\", df.head())\n",
    "print(\"\\nClass distribution:\\n\", df['target'].value_counts())\n",
    "\n",
    "# Split dataset: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize features for distance-based models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 2: Implement K-Nearest Neighbors (KNN)\n",
    "k_values = [1, 5, 11, 15, 21]\n",
    "knn_accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    knn_accuracies.append(acc)\n",
    "    print(f\"KNN - K = {k}, Accuracy = {acc:.4f}\")\n",
    "\n",
    "# Step 3: Implement Radius Neighbors (RNN)\n",
    "radius_values = [350, 400, 450, 500, 550, 600]\n",
    "rnn_accuracies = []\n",
    "\n",
    "for r in radius_values:\n",
    "    rnn = RadiusNeighborsClassifier(radius=r, outlier_label=-1)\n",
    "    rnn.fit(X_train_scaled, y_train)\n",
    "    y_pred = rnn.predict(X_test_scaled)\n",
    "    # Exclude outliers (-1) from accuracy calculation\n",
    "    valid_idx = y_pred != -1\n",
    "    acc = accuracy_score(y_test[valid_idx], y_pred[valid_idx]) if valid_idx.any() else 0\n",
    "    rnn_accuracies.append(acc)\n",
    "    print(f\"RNN - Radius = {r}, Accuracy = {acc:.4f}\")\n",
    "\n",
    "# Step 4: Visualize and Compare Results\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# KNN accuracy plot\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(k_values, knn_accuracies, marker='o')\n",
    "plt.title('KNN Accuracy vs. K Value')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "# RNN accuracy plot\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(radius_values, rnn_accuracies, marker='o', color='orange')\n",
    "plt.title('RNN Accuracy vs. Radius Value')\n",
    "plt.xlabel('Radius')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Performance Summary Table\n",
    "summary_df = pd.DataFrame({\n",
    "    'KNN_k': k_values,\n",
    "    'KNN_Accuracy': knn_accuracies,\n",
    "    'RNN_Radius': radius_values,\n",
    "    'RNN_Accuracy': rnn_accuracies\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance Summary Table:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Step 6: Discussion (Markdown cell recommended in notebook)\n",
    "discussion = \"\"\"\n",
    "Observations:\n",
    "- KNN accuracy stabilizes as k increases. Small k may overfit, large k can generalize better.\n",
    "- RNN accuracy depends heavily on radius. Too small: no neighbors; too large: noise impacts accuracy.\n",
    "- KNN generally provides more stable results; RNN is sensitive to radius selection.\n",
    "\n",
    "When to Use:\n",
    "- KNN is preferred for evenly distributed, dense datasets.\n",
    "- RNN can be used for datasets with variable density, but requires careful radius tuning.\n",
    "\"\"\"\n",
    "print(discussion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
